{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - model Tuning - 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('D:\\Ai-Learning\\Animal_Dataset.zip','r') as zip_ref:\n",
    "    zip_ref.extractall('D:\\Ai-Learning\\Animal_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, shear_range=0.2)\n",
    "test_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1238 images belonging to 4 classes.\n",
      "Found 326 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.flow_from_directory(\"D:\\Ai-Learning\\Animal_Dataset\\dataset\\Training\", target_size=(224,224), batch_size=27, class_mode='categorical')\n",
    "test = test_gen.flow_from_directory(\"D:\\Ai-Learning\\Animal_Dataset\\dataset\\Testing\", target_size=(224,224), batch_size=27, class_mode='categorical')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnedmodel = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add more layers to tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnedmodel.add(Convolution2D(27, (3,3), activation='relu', input_shape=(224,224,3)))\n",
    "tunnedmodel.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "tunnedmodel.add(Convolution2D(35, (3,3)))\n",
    "tunnedmodel.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "tunnedmodel.add(Convolution2D(25, (3,3)))\n",
    "tunnedmodel.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "tunnedmodel.add(Flatten())\n",
    "\n",
    "tunnedmodel.add(Dense(156, activation='relu'))\n",
    "tunnedmodel.add(Dense(75, activation='relu'))\n",
    "tunnedmodel.add(Dense(24, activation='relu'))\n",
    "tunnedmodel.add(Dense(14, activation='relu'))\n",
    "\n",
    "tunnedmodel.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnedmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "46/46 [==============================] - 37s 795ms/step - loss: 1.3330 - accuracy: 0.3522 - val_loss: 1.2175 - val_accuracy: 0.4080\n",
      "Epoch 2/25\n",
      "46/46 [==============================] - 35s 766ms/step - loss: 1.1663 - accuracy: 0.4669 - val_loss: 0.9194 - val_accuracy: 0.6350\n",
      "Epoch 3/25\n",
      "46/46 [==============================] - 36s 785ms/step - loss: 0.9350 - accuracy: 0.6074 - val_loss: 0.8160 - val_accuracy: 0.6687\n",
      "Epoch 4/25\n",
      "46/46 [==============================] - 37s 803ms/step - loss: 0.8717 - accuracy: 0.6551 - val_loss: 0.7497 - val_accuracy: 0.7577\n",
      "Epoch 5/25\n",
      "46/46 [==============================] - 36s 770ms/step - loss: 0.7436 - accuracy: 0.7149 - val_loss: 0.6824 - val_accuracy: 0.7669\n",
      "Epoch 6/25\n",
      "46/46 [==============================] - 36s 778ms/step - loss: 0.6398 - accuracy: 0.7649 - val_loss: 0.5885 - val_accuracy: 0.7699\n",
      "Epoch 7/25\n",
      "46/46 [==============================] - 40s 858ms/step - loss: 0.5561 - accuracy: 0.7868 - val_loss: 0.3920 - val_accuracy: 0.8436\n",
      "Epoch 8/25\n",
      "46/46 [==============================] - 36s 769ms/step - loss: 0.4387 - accuracy: 0.8465 - val_loss: 0.2478 - val_accuracy: 0.9264\n",
      "Epoch 9/25\n",
      "46/46 [==============================] - 38s 825ms/step - loss: 0.3404 - accuracy: 0.8805 - val_loss: 0.1809 - val_accuracy: 0.9417\n",
      "Epoch 10/25\n",
      "46/46 [==============================] - 36s 768ms/step - loss: 0.2438 - accuracy: 0.9111 - val_loss: 0.1716 - val_accuracy: 0.9417\n",
      "Epoch 11/25\n",
      "46/46 [==============================] - 35s 768ms/step - loss: 0.1954 - accuracy: 0.9370 - val_loss: 0.0763 - val_accuracy: 0.9785\n",
      "Epoch 12/25\n",
      "46/46 [==============================] - 35s 756ms/step - loss: 0.1359 - accuracy: 0.9604 - val_loss: 0.0415 - val_accuracy: 0.9908\n",
      "Epoch 13/25\n",
      "46/46 [==============================] - 35s 763ms/step - loss: 0.1142 - accuracy: 0.9669 - val_loss: 0.1458 - val_accuracy: 0.9509\n",
      "Epoch 14/25\n",
      "46/46 [==============================] - 35s 762ms/step - loss: 0.0830 - accuracy: 0.9766 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "46/46 [==============================] - 36s 781ms/step - loss: 0.0375 - accuracy: 0.9903 - val_loss: 0.0223 - val_accuracy: 0.9969\n",
      "Epoch 16/25\n",
      "46/46 [==============================] - 37s 798ms/step - loss: 0.1116 - accuracy: 0.9596 - val_loss: 0.1059 - val_accuracy: 0.9755\n",
      "Epoch 17/25\n",
      "46/46 [==============================] - 36s 770ms/step - loss: 0.0986 - accuracy: 0.9766 - val_loss: 0.0206 - val_accuracy: 0.9969\n",
      "Epoch 18/25\n",
      "46/46 [==============================] - 36s 790ms/step - loss: 0.0476 - accuracy: 0.9822 - val_loss: 0.0190 - val_accuracy: 0.9969\n",
      "Epoch 19/25\n",
      "46/46 [==============================] - 34s 741ms/step - loss: 0.0245 - accuracy: 0.9911 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "46/46 [==============================] - 35s 759ms/step - loss: 0.0298 - accuracy: 0.9911 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "46/46 [==============================] - 34s 735ms/step - loss: 0.0280 - accuracy: 0.9927 - val_loss: 0.0155 - val_accuracy: 0.9969\n",
      "Epoch 22/25\n",
      "46/46 [==============================] - 36s 780ms/step - loss: 0.0376 - accuracy: 0.9871 - val_loss: 0.0173 - val_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "46/46 [==============================] - 37s 807ms/step - loss: 0.1122 - accuracy: 0.9620 - val_loss: 0.0640 - val_accuracy: 0.9724\n",
      "Epoch 24/25\n",
      "46/46 [==============================] - 33s 722ms/step - loss: 0.0626 - accuracy: 0.9798 - val_loss: 0.0361 - val_accuracy: 0.9908\n",
      "Epoch 25/25\n",
      "46/46 [==============================] - 34s 729ms/step - loss: 0.0291 - accuracy: 0.9895 - val_loss: 0.0091 - val_accuracy: 0.9969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26af376afb0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tunnedmodel.fit(train, validation_data=test, epochs=25, batch_size=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnedmodel.save('D:/Ai-Learning/tunnedmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crow\n",
      "crow\n",
      "elephant\n",
      "elephant\n"
     ]
    }
   ],
   "source": [
    "# model - testing\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "output = ['bear', 'crow', 'elephant', 'rat']\n",
    "def func(str):\n",
    "    img = image.load_img(str, target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "img = func('DataSets/AnimalPredict/bear.jpg')\n",
    "print(output[np.argmax(tunnedmodel.predict(img))])\n",
    "\n",
    "img = func('DataSets/AnimalPredict/crow.jpg')\n",
    "print(output[np.argmax(tunnedmodel.predict(img))])\n",
    "\n",
    "img = func('DataSets/AnimalPredict/elephant.jpg')\n",
    "print(output[np.argmax(tunnedmodel.predict(img))])\n",
    "\n",
    "img = func('DataSets/AnimalPredict/rat.jpg')\n",
    "print(output[np.argmax(tunnedmodel.predict(img))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tunning - with\n",
    "#### 1. Dropout\n",
    "#### 2. Batch-Normalization\n",
    "#### 3. Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(Convolution2D(27, (3,3), activation='relu', input_shape=(224,224,3)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Convolution2D(35, (3,3)))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Convolution2D(25, (3,3)))\n",
    "model2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Flatten())\n",
    "\n",
    "model2.add(Dense(156, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(75, activation='relu'))\n",
    "model2.add(Dense(24, activation='relu'))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Dense(14, activation='relu'))\n",
    "\n",
    "model2.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - 53s 1s/step - loss: 1.2781 - accuracy: 0.4548 - val_loss: 1.4190 - val_accuracy: 0.2239\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 55s 1s/step - loss: 0.9548 - accuracy: 0.6648 - val_loss: 1.3380 - val_accuracy: 0.3190\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 58s 1s/step - loss: 0.7611 - accuracy: 0.7456 - val_loss: 1.6123 - val_accuracy: 0.2270\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 55s 1s/step - loss: 0.6199 - accuracy: 0.7964 - val_loss: 1.6202 - val_accuracy: 0.2362\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 57s 1s/step - loss: 0.5350 - accuracy: 0.8086 - val_loss: 1.5443 - val_accuracy: 0.3006\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 60s 1s/step - loss: 0.4262 - accuracy: 0.8562 - val_loss: 1.3575 - val_accuracy: 0.3528\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 59s 1s/step - loss: 0.3527 - accuracy: 0.8772 - val_loss: 1.1959 - val_accuracy: 0.5123\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 56s 1s/step - loss: 0.3189 - accuracy: 0.8974 - val_loss: 1.6118 - val_accuracy: 0.3190\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 57s 1s/step - loss: 0.2497 - accuracy: 0.9225 - val_loss: 1.5495 - val_accuracy: 0.5092\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 52s 1s/step - loss: 0.2217 - accuracy: 0.9313 - val_loss: 0.9642 - val_accuracy: 0.7025\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 65s 1s/step - loss: 0.1947 - accuracy: 0.9386 - val_loss: 0.6557 - val_accuracy: 0.7761\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 59s 1s/step - loss: 0.1627 - accuracy: 0.9491 - val_loss: 0.7884 - val_accuracy: 0.6994\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 56s 1s/step - loss: 0.1027 - accuracy: 0.9701 - val_loss: 0.3831 - val_accuracy: 0.9080\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 56s 1s/step - loss: 0.1033 - accuracy: 0.9717 - val_loss: 0.7739 - val_accuracy: 0.8466\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 58s 1s/step - loss: 0.0976 - accuracy: 0.9685 - val_loss: 0.5069 - val_accuracy: 0.9080\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 59s 1s/step - loss: 0.0860 - accuracy: 0.9709 - val_loss: 7.4178 - val_accuracy: 0.6902\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 55s 1s/step - loss: 0.1835 - accuracy: 0.9370 - val_loss: 3.7957 - val_accuracy: 0.8466\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 55s 1s/step - loss: 0.1374 - accuracy: 0.9515 - val_loss: 10.7470 - val_accuracy: 0.7515\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 60s 1s/step - loss: 0.1013 - accuracy: 0.9669 - val_loss: 11.8121 - val_accuracy: 0.4816\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26a8145e980>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystop = EarlyStopping(monitor='accuracy',patience=5)\n",
    "model2.fit(train, validation_data=test, epochs=100, batch_size=27 ,callbacks=mystop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bear\n",
      "bear\n",
      "bear\n",
      "bear\n"
     ]
    }
   ],
   "source": [
    "img = func('DataSets/AnimalPredict/bear.jpg')\n",
    "print(output[np.argmax(model2.predict(img))])\n",
    "\n",
    "img = func('DataSets/AnimalPredict/crow.jpg')\n",
    "print(output[np.argmax(model2.predict(img))])\n",
    "\n",
    "img = func('DataSets/AnimalPredict/elephant.jpg')\n",
    "print(output[np.argmax(model2.predict(img))])\n",
    "\n",
    "img = func('DataSets/AnimalPredict/rat.jpg')\n",
    "print(output[np.argmax(model2.predict(img))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('D:/Ai-Learning/tunnedmodel2.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
